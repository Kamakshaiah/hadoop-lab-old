package tv.floe.caduceus.hadoop.movingaverage;

import java.util.ArrayList;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

/**
 * Map Reduce job demonstrating time series data being processed without
 * secondary sort.
 * 
 * @author jpatterson
 * 
 */
public class NoShuffleSort_MovingAverageJob extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {

		System.out.println("\n\nNoShuffleSort_MovingAverageJob\n");

		JobConf conf = new JobConf(getConf(),
				NoShuffleSort_MovingAverageJob.class);
		conf.setJobName("NoShuffleSort_MovingAverageJob");

		// since we only want to group by stock, we'll use the Text Class
		conf.setMapOutputKeyClass(Text.class);
		conf.setMapOutputValueClass(TimeseriesDataPoint.class);

		conf.setMapperClass(NoShuffleSort_MovingAverageMapper.class);
		conf.setReducerClass(NoShuffleSort_MovingAverageReducer.class);

		List<String> other_args = new ArrayList<String>();
		for (int i = 0; i < args.length; ++i) {
			try {
				if ("-m".equals(args[i])) {

					conf.setNumMapTasks(Integer.parseInt(args[++i]));

				} else if ("-r".equals(args[i])) {

					conf.setNumReduceTasks(Integer.parseInt(args[++i]));

				} else if ("-windowSize".equals(args[i])) {

					conf.set(
							"tv.floe.caduceus.hadoop.movingaverage.windowSize",
							args[++i]);

				} else if ("-windowStepSize".equals(args[i])) {

					conf
							.set(
									"tv.floe.caduceus.hadoop.movingaverage.windowStepSize",
									args[++i]);

				} else {

					other_args.add(args[i]);

				}
			} catch (NumberFormatException except) {
				System.out.println("ERROR: Integer expected instead of "
						+ args[i]);
				return printUsage();
			} catch (ArrayIndexOutOfBoundsException except) {
				System.out.println("ERROR: Required parameter missing from "
						+ args[i - 1]);
				return printUsage();
			}
		}
		// Make sure there are exactly 2 parameters left.
		if (other_args.size() != 2) {
			System.out.println("ERROR: Wrong number of parameters: "
					+ other_args.size() + " instead of 2.");
			return printUsage();
		}

		conf.setInputFormat(TextInputFormat.class);

		conf.setOutputFormat(TextOutputFormat.class);
		conf.setCompressMapOutput(true);

		FileInputFormat.setInputPaths(conf, other_args.get(0));
		FileOutputFormat.setOutputPath(conf, new Path(other_args.get(1)));

		JobClient.runJob(conf);

		return 0;
	}

	static int printUsage() {
		System.out
				.println("NoShuffleSort_MovingAverageJob [-m <maps>] [-r <reduces>] <input> <output>");
		ToolRunner.printGenericCommandUsage(System.out);
		return -1;
	}

	public static void main(String[] args) throws Exception {

		int res = ToolRunner.run(new Configuration(),
				new NoShuffleSort_MovingAverageJob(), args);
		System.exit(res);

	}

}



##### MAPPER ##########

package tv.floe.caduceus.hadoop.movingaverage;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
import org.apache.log4j.Logger;

public class NoShuffleSort_MovingAverageMapper  extends MapReduceBase implements Mapper<LongWritable, Text, Text, TimeseriesDataPoint> 
{

	   
	static enum Timeseries_Counters { BAD_PARSE, BAD_LOOKUP };

	   
	   private JobConf configuration;
	   private final Text key = new Text();
	   private final TimeseriesDataPoint val = new TimeseriesDataPoint();
	   

	   private static final Logger logger = Logger.getLogger( NoShuffleSort_MovingAverageMapper.class );

	   
	   public void close() {
		   
		   
	   }
	   
	   public void configure(JobConf conf) {
	      this.configuration = conf;
	      
	   }
	   
	   
	   
	   
	   @Override
	   public void map(LongWritable inkey, Text value, OutputCollector<Text, TimeseriesDataPoint> output, Reporter reporter) throws IOException {

	      String line = value.toString();
	      
	      YahooStockDataPoint rec = YahooStockDataPoint.parse( line );
	      
	      if (rec != null) {
	    	  
	    		  // set both parts of the key
	    		  key.set( rec.stock_symbol );
	    	      
	    		  val.fValue = rec.getAdjustedClose();
	    	      val.lDateTime = rec.date;
	    	      
	    	      // now that its parsed, we send it through the shuffle for sort, onto reducers
	    	      output.collect(key, val);
	         
	      } else {
	    	  
	    	  reporter.incrCounter( Timeseries_Counters.BAD_PARSE, 1 );
	    	  
	      }
	    	  
	   }

}

##### REDUCER ############


package tv.floe.caduceus.hadoop.movingaverage;

import java.io.IOException;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.PriorityQueue;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter;

/**
 * 
 * NoShuffleSort_MovingAverageReducer
 * 
 * In this version of the reducer the points do not arrive in pre-sorted form so
 * we have to maintain an in-memory queue to sort these points
 * 
 * 
 */

public class NoShuffleSort_MovingAverageReducer extends MapReduceBase implements
		Reducer<Text, TimeseriesDataPoint, Text, Text> {

	static enum PointCounters {
		POINTS_SEEN, POINTS_ADDED_TO_WINDOWS, MOVING_AVERAGES_CALCD
	};

	private JobConf configuration;

	@Override
	public void configure(JobConf job) {

		this.configuration = job;

	} // configure()

	public void reduce(Text key, Iterator<TimeseriesDataPoint> values,
			OutputCollector<Text, Text> output, Reporter reporter)
			throws IOException {

		TimeseriesDataPoint next_point;
		float point_sum = 0;
		float moving_avg = 0;

		// make static
		long day_in_ms = 24 * 60 * 60 * 1000;

		// should match the width of your training samples sizes
		int iWindowSizeInDays = this.configuration.getInt(
				"tv.floe.caduceus.hadoop.movingaverage.windowSize", 30);
		int iWindowStepSizeInDays = this.configuration.getInt(
				"tv.floe.caduceus.hadoop.movingaverage.windowStepSize", 1);

		long iWindowSizeInMS = iWindowSizeInDays * day_in_ms; // =
																// this.configuration.getInt("tv.floe.examples.mr.sax.windowSize",
																// 14 );
		long iWindowStepSizeInMS = iWindowStepSizeInDays * day_in_ms; // =
																		// this.configuration.getInt("tv.floe.examples.mr.sax.windowStepSize",
																		// 7 );

		Text out_key = new Text();
		Text out_val = new Text();

		SlidingWindow sliding_window = new SlidingWindow(iWindowSizeInMS,
				iWindowStepSizeInMS, day_in_ms);

		PriorityQueue<TimeseriesDataPoint> oPointHeapNew = new PriorityQueue<TimeseriesDataPoint>();

		while (values.hasNext()) {

			next_point = values.next();

			// we need to copy the points into new objects since MR re-uses k/v
			// pairs
			// to avoid GC churn
			TimeseriesDataPoint point_copy = new TimeseriesDataPoint();
			point_copy.copy(next_point);

			oPointHeapNew.add(point_copy);

		} // while

		while (oPointHeapNew.isEmpty() == false) {

			reporter.incrCounter(PointCounters.POINTS_ADDED_TO_WINDOWS, 1);

			next_point = oPointHeapNew.poll();

			try {
				sliding_window.AddPoint(next_point);
			} catch (Exception e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}

			if (sliding_window.WindowIsFull()) {

				reporter.incrCounter(PointCounters.MOVING_AVERAGES_CALCD, 1);

				LinkedList<TimeseriesDataPoint> oWindow = sliding_window
						.GetCurrentWindow();

				String strBackDate = oWindow.getLast().getDate();

				// ---------- compute the moving average here -----------

				out_key.set("Group: " + key.toString() + ", Date: "
						+ strBackDate);

				point_sum = 0;

				for (int x = 0; x < oWindow.size(); x++) {

					point_sum += oWindow.get(x).fValue;

				} // for

				moving_avg = point_sum / oWindow.size();

				out_val.set("Moving Average: " + moving_avg);

				output.collect(out_key, out_val);

				// 2. step window forward

				sliding_window.SlideWindowForward();

			}

		}

	} // reduce

}