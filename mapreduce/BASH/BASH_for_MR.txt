#------------------
##### mapper #####
#------------------

#!/bin/bash

#The mapper will read one line at a time and then break it into words (defined by a fixed InputFormat for streaming)

while read line
do
for word in $line
do
if [ -n $word ]
then
wcount=`echo $word | wc -m`;
wlength=`expr $wcount - 1`;
letter=`echo $word | head -c1`;
echo -e "$letter\t$wlength";
fi
done
done 

#------------------
##### reducer #####
#------------------

#!/bin/bash

#Remember that the framework will sort the output from mappers based on the Key.
#Note that the input to a reducer will be of the form (key, value) and not (key, <list of values>). This is unlike the input that is usually passed to a Reducer written in Java.

lastkey="";
count=0;
total=0;
iteration=1

while read line

do
newkey=`echo $line | awk '{print $1}'`
value=`echo $line | awk '{print $2}'`

if [ "$iteration" == "1" ]
then
lastkey=$newkey;
iteration=`expr $iteration + 1`;
fi

if [[ "$lastkey" != "$newkey" ]]
then
average=`echo "scale=5;$total / $count" | bc`;
echo -e "$lastkey\t$average"
count=0;
lastkey=$newkey;
total=0;
average=0;
fi

total=`expr $total + $value`;
lastkey=$newkey;
count=`expr $count + 1`;

done 

#--------------------
##### execution #####
#--------------------

hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming*.jar -input /input -output /output -mapper mapper.sh -reducer reducer.sh -file avgwl/mapper.sh -file avgwl/reducer.sh 